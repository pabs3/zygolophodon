#!/usr/bin/env python3

# Copyright Â© 2022-2025 Jakub Wilk <jwilk@jwilk.net>
# SPDX-License-Identifier: MIT

import abc
import argparse
import contextlib
import errno
import gzip
import html.parser
import http.client
import inspect
import io
import json
import os
import re
import shutil
import signal
import socket
import subprocess
import sys
import textwrap
import types
import urllib.parse
import urllib.request

int(0_0)  # Python >= 3.6 is required

__version__ = '0'  # not released yet

text_width = int(os.getenv('ZYGOLOPHODON_COLUMNS', '78'))

class Symbol():

    def __init__(self, name):
        name = name.upper().replace(' ', '_')
        text = os.getenv(f'ZYGOLOPHODON_{name}', '*')
        match = re.fullmatch('(.*):([0-9]+)', text)
        if match:
            (text, width) = match.groups()
            width = int(width)
        else:
            width = 1
        self._text = text
        self.width = width

    def __str__(self):
        return self._text

prog = argparse.ArgumentParser().prog

def find_command(command):
    if shutil.which(command):
        return command
    return None

def fatal(msg):
    print(f'{prog}: {msg}', file=sys.stderr)
    sys.exit(1)

class StdOut(io.TextIOBase):

    def _install_pager(self):
        if not sys.__stdout__.isatty():
            return
        cmdline = (os.environ.get('PAGER')
            or find_command('pager')  # Debian:
            # https://www.debian.org/doc/debian-policy/ch-customized-programs.html#editors-and-pagers
            or 'more'  # POSIX:
            # https://pubs.opengroup.org/onlinepubs/007904975/utilities/man.html#tag_04_85_08
        )
        if cmdline == 'cat':
            return
        env = None
        if 'LESS' not in os.environ:
            env = dict(env or os.environ, LESS='-FXK')
        self._pager = subprocess.Popen(cmdline, shell=True, stdin=subprocess.PIPE, env=env)  # pylint: disable=consider-using-with
        self._stdout = io.TextIOWrapper(self._pager.stdin,
            encoding=sys.__stdout__.encoding,
            errors=sys.__stdout__.errors,
            line_buffering=True,
        )

    def __init__(self):
        super().__init__()
        self._newlines = 0
        self._pager = None
        self._stdout = sys.__stdout__
        self._install_pager()

    def _get_fp(self):
        if UserAgent.debug_level:
            # Redirect http.client's debug messages to stderr:
            for frameinfo in inspect.stack(context=0):
                if frameinfo.filename == http.client.__file__:
                    return sys.__stderr__
        return self._stdout

    def write(self, s):
        fp = self._get_fp()
        if fp is self._stdout:
            if s == '':
                return
            if s == '\n':
                if self._newlines == 2:
                    return
                self._newlines += 1
            else:
                self._newlines = int(s[-1] == '\n')
        fp.write(s)

    def flush(self):
        self._get_fp().flush()

    def isatty(self):
        return sys.__stdout__.isatty()

    def __exit__(self, exc_type, exc_value, traceback):
        if self._pager:
            self._pager.__exit__(exc_type, exc_value, traceback)
            if exc_type is None and self._pager.returncode != 0:
                raise RuntimeError('pager failed')
            self._pager = None
            self._stdout = None

    @staticmethod
    @contextlib.contextmanager
    def install():
        assert sys.stdout is sys.__stdout__
        try:
            with StdOut() as sys.stdout:
                yield
        finally:
            sys.stdout = sys.__stdout__

def fmt_url_error(exc):
    if isinstance(exc, urllib.error.HTTPError):
        return str(exc)
    exc = exc.reason
    if isinstance(exc, socket.gaierror):
        for key, value in vars(socket).items():
            if key[:4] == 'EAI_' and value == exc.errno:
                return f'[{key}] {exc.strerror}'
    if isinstance(exc, ConnectionError):
        try:
            ec = errno.errorcode[exc.errno]
        except LookupError:
            pass
        else:
            return f'[{ec}] {exc.strerror}'
    return str(exc)

class UserAgent():

    headers = {
        'User-Agent': 'zygolophodon (https://github.com/jwilk/zygolophodon)',
        'Accept-Encoding': 'gzip',
    }
    debug_level = 0

    @classmethod
    def _build_opener(cls):
        # Work-around for <https://github.com/python/cpython/issues/99352>
        # ("urllib.request.urlopen() no longer respects the
        # http.client.HTTPConnection.debuglevel").
        # TODO: Get rid of this once Python < 3.12 is no longer supported.
        handlers = [
            Handler(debuglevel=cls.debug_level)
            for Handler in [urllib.request.HTTPHandler, urllib.request.HTTPSHandler]
        ]
        return urllib.request.build_opener(*handlers)

    @classmethod
    def get(cls, url):
        headers = dict(cls.headers)
        request = urllib.request.Request(url, headers=headers)
        opener = cls._build_opener()
        try:
            response = opener.open(request)
        except urllib.error.URLError as exc:
            msg = fmt_url_error(exc)
            fatal(f'<{url}>: {msg}')
        return Response(response)

class Response():

    def __init__(self, response):
        with response:
            content_encoding = response.getheader('Content-Encoding', 'identity')
            data = response.read()
        if content_encoding == 'gzip':
            data = gzip.decompress(data)
        elif content_encoding == 'identity':
            pass
        else:
            raise RuntimeError(f'unexpected Content-Encoding: {content_encoding!r}')
        self.data = data
        self.headers = response.headers

    @property
    def json(self):
        return json.loads(self.data, object_hook=Dict)

wget = UserAgent.get

def wget_json(url):
    return wget(url).json

class Dict(dict):
    __getattr__ = dict.__getitem__

def fmt_url(url):
    if sys.stdout.isatty():
        return re.sub('(.)', r'_\b\1', url)
    return url

def fmt_user(account):
    return f'{account.display_name} <{fmt_url(account.url)}>'.lstrip()

def fmt_date(d):
    d = re.sub(r'[.]\d+', '', d)
    d = d.replace('T', ' ')
    return d

class HTMLParser(html.parser.HTMLParser):

    def __init__(self):
        super().__init__()
        class state:
            paras = []
            text = ''
            a_text = ''
            a_href = None
            a_depth = 0
            footnotes = {}
        self.z_state = state

    # FIXME: Add proper suport for:
    # * <ol>, <ul>, <li>
    # * <blockquote>
    # * <pre>

    def handle_starttag(self, tag, attrs):
        st = self.z_state
        if tag in {'p', 'ol', 'ul', 'blockquote', 'pre'}:
            while st.a_depth > 0:
                self.handle_endtag('a')
            if st.text:
                st.paras += [st.text]
                st.text = ''
            return
        if tag == 'br':
            if st.a_depth > 0:
                st.a_text += ' '
            else:
                st.text += '\n'
            return
        if tag == 'a':
            if st.a_depth == 0:
                st.a_href = dict(attrs).get('href', '')
            st.a_depth += 1
            return

    def handle_endtag(self, tag):
        st = self.z_state
        if tag == 'a':
            if st.a_depth > 0:
                st.a_depth -= 1
            if st.a_depth == 0:
                text = st.a_text
                href = st.a_href
                if re.fullmatch(r'[@#]\w+', text) and st.footnotes.get(text, href) == href:
                    cc = '\N{DC1}\N{DC2}'[text[0] != '@']
                    st.text += f'{cc}{text[1:]}\N{ETX}'
                    st.footnotes[text] = href
                elif re.fullmatch(r'#\w+', text):
                    st.text += '\N{DC2}' + text[1:]
                else:
                    if st.a_href in {text, f'http://{text}', f'https://{text}'}:
                        text = ''
                    else:
                        text = f'[{text}]'
                    st.text += f'{text}\N{STX}{st.a_href}\N{ETX}'
                st.a_href = ''
                st.a_text = ''
            return
        if tag == 'li':
            if st.a_depth > 0:
                st.a_text += ' '
            else:
                st.text += '\n'
            return

    def handle_data(self, data):
        st = self.z_state
        data = re.sub('[\N{STX}\N{ETX}\N{DC1}\N{DC2}]', '\N{REPLACEMENT CHARACTER}', data)
        data = re.sub(r'\s+', ' ', data)
        if st.a_depth > 0:
            st.a_text += data
        else:
            st.text += data

    def close(self):
        super().close()
        self.handle_starttag('p', {})

    if sys.version_info < (3, 10):
        def error(self, message):
            # hopefully not reachable
            raise RuntimeError(message)

link_symbol = Symbol('link symbol')

def fmt_html(data):
    parser = HTMLParser()
    parser.feed(data)
    parser.close()
    lines = []
    for para in parser.z_state.paras:
        for line in para.splitlines():
            lines += wrap_text(line)
        lines += ['']
    text = str.join('\n', lines)
    def repl(match):
        (url, foottype, footnote) = match.groups()
        if url is not None:
            url = fmt_url(url)
            return f'<{url}>'
        else:
            # FIXME: The replacement text is one character shorter
            # than the input, so it slightly disturbs text wrapping.
            prefix = '@#'[foottype != '\N{DC1}']
            footnote = prefix + footnote
            return fmt_url(footnote)
    text = re.sub('\N{STX}(.*?)\N{ETX}|([\N{DC1}\N{DC2}])(\\w+)\N{ETX}', repl, text, flags=re.DOTALL)
    lines = [text]
    if parser.z_state.footnotes:
        for footnote, url in parser.z_state.footnotes.items():
            url = fmt_url(url)
            lines += [f'{link_symbol} {footnote}: {url}']
    return str.join('\n', lines)

class VersionAction(argparse.Action):
    '''
    argparse --version action
    '''

    def __init__(self, option_strings, dest=argparse.SUPPRESS):
        super().__init__(
            option_strings=option_strings,
            dest=dest,
            nargs=0,
            help='show version information and exit'
        )

    def __call__(self, parser, namespace, values, option_string=None):
        print(f'{parser.prog} {__version__}')
        print('+ Python {0}.{1}.{2}'.format(*sys.version_info))  # pylint: disable=consider-using-f-string
        parser.exit()

class AddrParser():

    _groups = set()

    def __init__(self, *templates, ident_regexp, discard_prefixes=()):
        self.templates = []
        self._ident_regexp = ident_regexp
        self._regexps = []
        self._discard_prefixes = discard_prefixes
        for template in templates:
            if template[0] == '/':
                template = f'https://DOMAIN{template}'
            self._add_template(template)
        del self._discard_prefixes

    def _add_template(self, template):
        self.templates += [template]
        group2regexp = dict(
            domain=r'[^@/?#\0-\40]+',
            user=r'[^/?#\0-\40]+',
            ident=self._ident_regexp
        )
        def repl(match):
            s = match.group()
            if match.start() == 0 and s == 'https':
                return s
            if s.isupper():
                group = s.lower()
                if group == 'nnnnnn':
                    group = 'ident'
                regexp = group2regexp[group]
            else:
                group = s
                regexp = re.escape(s)
            self._groups.add(group)
            return f'(?P<{group}>{regexp})'
        discard = self._discard_prefixes
        if discard:
            discard_re = str.join('|', map(re.escape, discard))
            discard_re = f'(?:{discard_re})'
            template = template.replace('/DOMAIN/', f'/DOMAIN/(?:{discard_re}/)*')
        regexp = re.sub(r'(?<![:|\w])\w+', repl, template)
        regexp = re.compile(regexp)
        self._regexps += [regexp]

    def parse(self, url):
        for regexp in self._regexps:
            match = re.fullmatch(regexp, url)
            if match is not None:
                break
        else:
            return None
        data = {group: None for group in self._groups}
        data.update(match.groupdict())
        return types.SimpleNamespace(**data)

def pint(s):
    n = int(s)
    if n > 0:
        return n
    raise ValueError
pint.__name__ = 'positive int'

def xmain():
    ap = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)
    if sys.version_info < (3, 10):
        # https://bugs.python.org/issue9694
        ap._optionals.title = 'options'  # pylint: disable=protected-access
    ap.add_argument('--version', action=VersionAction)
    default_limit = 40
    ap.add_argument('--limit', metavar='N', type=pint, default=default_limit,
        help=f'request at most N posts (default: {default_limit})'
    )
    ap.add_argument('--debug-http', action='store_true', help=argparse.SUPPRESS)
    addr_help = []
    for instance_type in Instance.types:
        for template in instance_type.addr_parser.templates:
            line = template
            if instance_type is not Mastodon:
                line += f' ({instance_type.__name__})'
            addr_help += [line]
    addr_help = str.join('\n', addr_help)
    ap.add_argument('addr', metavar='ADDRESS', help=addr_help)
    opts = ap.parse_args()
    if opts.debug_http:
        UserAgent.debug_level = 1
    addr = opts.addr
    if '/' in addr:
        # strip URL fragment
        addr, _ = urllib.parse.urldefrag(addr)
    for instance_type in Instance.types:
        match = instance_type.parse_addr(addr)
        if match is not None:
            break
    else:
        ap.error('unsupported address')
    instance = Instance.connect(match.url)
    sys.stdout.flush()
    with StdOut.install():
        if match.ident is None:
            process_user(instance, match.user,
                replies=bool(match.with_replies),
                media=bool(match.media),
                limit=opts.limit,
            )
        else:
            process_post(instance, match.ident,
                with_replies=(opts.limit > 1 and not match.embed),
            )

def parse_links(s):
    data = {}
    regexp = re.compile(r'<([^>]+)>; rel="(\w+)"(?:, |\Z)')
    i = 0
    while i < len(s):
        match = regexp.match(s, i)
        if match is None:
            raise RuntimeError(f'cannot parse Link header field: {s!r}')
        (value, key) = match.groups()
        data[key] = value
        i = match.end()
    return data

def abstractattribute():
    return abc.abstractmethod(lambda: None)

class Instance(abc.ABC):

    types = []

    post_url_template = abstractattribute()

    ident_regexp = abstractattribute()

    addr_parser = abstractattribute()

    def __init__(self, url, data):
        self.data = data
        self.url = url
        self.api_url = f'{url}/api/v1'

    @classmethod
    def parse_addr(cls, addr):
        match = cls.addr_parser.parse(addr)  # pylint: disable=no-member
        if not match:
            return None
        if match.user is not None:
            match.user = urllib.parse.unquote(match.user)
        match.url = f'https://{match.domain}'
        del match.domain
        return match

    @staticmethod
    def connect(url):
        api = Mastodon(url, None).api_url
        data = wget_json(f'{api}/instance')
        inst_types = sorted(
            Instance.types,
            key=(lambda t: t.identify(data)),
            reverse=True,
        )
        inst_type = inst_types[0]
        return inst_type(url, data)

    @classmethod
    @abc.abstractmethod
    def identify(cls, data):
        pass

    def fetch_user_by_name(self, name):
        api = self.api_url
        q_name = urllib.parse.quote(name)
        return wget_json(f'{api}/accounts/lookup?acct={q_name}')

    def fetch_user_posts(self, user, *, limit=1e999, **params):
        api = self.api_url
        page_limit = 40
        params['limit'] = min(limit, page_limit)
        q_params = urllib.parse.urlencode(params).lower()
        url = f'{api}/accounts/{user.id}/statuses?{q_params}'
        while limit > 0:
            response = wget(url)
            posts = response.json
            self.fix_posts(posts)
            yield from posts
            limit -= len(posts)
            links = response.headers.get('Link', '')
            links = parse_links(links)
            next_url = links.get('next')
            if next_url is None:
                break
            if not url.startswith(f'{api}/'):
                raise RuntimeError(f'suspicious Link URL: {next_url!r}')
            url = re.sub(
                r'(?<=[?&]limit=)\d+(?=&|\Z)',
                str(min(limit, page_limit)),
                next_url
            )

    def fetch_post(self, post_id):
        api = self.api_url
        post = wget_json(f'{api}/statuses/{post_id}')
        self.fix_post(post)
        return post

    def fetch_post_context(self, post_id, ancestors=True, descendants=True):
        if not (ancestors or descendants):
            # shortcut:
            return Dict(ancestors=None, descendants=None)
        api = self.api_url
        context = wget_json(f'{api}/statuses/{post_id}/context')
        if ancestors:
            self.fix_posts(context.ancestors)
        else:
            context.ancestors = None
        if descendants:
            self.fix_posts(context.descendants)
        else:
            context.descendants = None
        return context

    def get_post_url(self, *, ident):
        template = self.post_url_template
        path = re.sub(r'\bIDENT\b', re.escape(ident), template)
        return f'{self.url}{path}'

    def get_fixed_post_url(self, url):
        return url

    def fix_post(self, post):
        irt_url = None
        if post.in_reply_to_id:
            irt_url = self.get_post_url(ident=post.in_reply_to_id)
        post.in_reply_to_url = irt_url
        if post.reblog:
            self.fix_post(post.reblog)
            if post.url == post.reblog.uri:
                # FIXME in Pleroma?
                # Why is the URL unhelpful?
                post.url = self.get_post_url(ident=post.id)
            if post.uri == post.reblog.uri:
                post.uri = None
            try:
                post.edited_at
            except KeyError:
                # FIXME in Pleroma?
                # Why is the attribute missing?
                post.edited_at = None
        post.url = self.get_fixed_post_url(post.url)

    def fix_posts(self, posts):
        for post in posts:
            self.fix_post(post)

    @classmethod
    def register(cls, instance_type):
        cls.types += [instance_type]
        return instance_type

@Instance.register
class Mastodon(Instance):

    # Codebase: https://github.com/mastodon/mastodon

    post_url_template = '/statuses/IDENT'

    ident_regexp = '[0-9]{17,18}'
    # Source: lib/mastodon/snowflake.rb
    #
    # Identifiers are decimal integers:
    #
    #    n = (t << 16) + r
    #
    # where
    #
    #    t is milliseconds since 1970;
    #    r are randomish lower bits.
    #
    # In practice, it's always
    # either 17 digits (until 2018)
    #     or 18 digits (2018-2453).
    #
    # $ export TZ=UTC0
    # $ qalc -t '"1970-01-01" + ((10 ** 16) >> 16) ms'
    # "1974-11-02T01:31:27"
    # $ qalc -t '"1970-01-01" + ((10 ** 17) >> 16) ms'
    # "2018-05-09T15:14:39"
    # $ qalc -t '"1970-01-01" + ((10 ** 18) >> 16) ms'
    # "2453-07-13T08:30:35"

    addr_parser = AddrParser(
        # mail-like
        '@USER@DOMAIN',
        'USER@DOMAIN',
        # user
        '/@USER',
        '/@USER/media',
        '/@USER/with_replies',
        # post
        '/@USER/NNNNNN',
        '/@USER/NNNNNN/embed',
        # legacy user-less post
        '/statuses/NNNNNN',
        # URI->URL redirects
        '/users/USER',
        '/users/USER/statuses/NNNNNN',
        #
        discard_prefixes={'deck', 'web'},
        ident_regexp=ident_regexp,
    )

    @classmethod
    def identify(cls, data):
        return 0

    def get_fixed_post_url(self, url):
        q_base_url = re.escape(self.url)
        match = re.fullmatch(q_base_url + '/users/([^/]+)/statuses/([0-9]+)/activity', url or '')
        if match:
            # FIXME in Mastodon?
            # Why is the URL unhelpful for reblogs?
            (user, ident) = match.groups()
            url = f'{self.url}/@{user}/{ident}'
        return url

@Instance.register
class Iceshrimp(Instance):

    # Codebase: https://iceshrimp.dev/

    post_url_template = '/notes/IDENT'

    ident_regexp = '[0-9a-z]{16,24}'
    # Source: packages/backend/src/misc/gen-id.ts
    #
    # Identifiers are in the form:
    #
    #    t || r
    #
    # where
    #
    #    t is milliseconds since 2000;
    #    r is randomish, configurable length 8-16.
    #
    # Both are in base-36.
    #
    # The docs say the timestamp is 8 chars long
    # (and the code indeed ensures it's _at least_ 8 chars),
    # but that'll only suffice until 2089.
    #
    # $ export TZ=UTC0
    # $ qalc -t '"2000-01-01" + (36 ** 8) ms'
    # "2089-05-24T17:38:22"

    addr_parser = AddrParser(
        '/notes/IDENT',
        ident_regexp=ident_regexp,
    )

    @classmethod
    def identify(cls, data):
        if re.search(r'\bIceshrimp\b', data.version):
            return 1
        return -1

@Instance.register
class Pleroma(Instance):

    # Codebase: https://git.pleroma.social/pleroma/pleroma

    post_url_template = '/notice/IDENT'

    ident_regexp = '[0-9a-zA-Z]{18}'
    # Source: https://git.pleroma.social/pleroma/flake_id
    #
    # Identifiers are base-62 integers:
    #
    #    n = (t << 64) + r
    #
    # where
    #
    #    t is milliseconds since 1970;
    #    r are randomish lower bits.
    #
    # In practice, it's always 18 digits (until 2284).
    #
    # $ export TZ=UTC0
    # $ qalc -t '"1970-01-01" + ((62 ** 17) >> 64) ms'
    # "1975-01-29T11:50:12"
    # $ qalc -t '"1970-01-01" + ((62 ** 18) >> 64) ms'
    # "2284-10-19T13:56:44"

    addr_parser = AddrParser(
        '/notice/IDENT',
        ident_regexp=ident_regexp,
    )

    @classmethod
    def identify(cls, data):
        try:
            data.pleroma
        except KeyError:
            return -1
        return 1

def process_user(instance, username, *, replies=False, media=False, limit=1e999):
    user = instance.fetch_user_by_name(username)
    print('User:', fmt_user(user))
    if user.note:
        print()
        print(fmt_html(user.note))
    if not (media or replies):
        posts = instance.fetch_user_posts(user, limit=limit, pinned=True)
        limit -= print_posts(posts, pinned=True)
    params = types.SimpleNamespace()
    if media:
        params.only_media=True
    else:
        params.exclude_replies = not replies
    posts = instance.fetch_user_posts(user, limit=limit, **vars(params))
    print_posts(posts)

def process_post(instance, ident, *, with_replies=True):
    post = instance.fetch_post(ident)
    print_post(post)
    if with_replies:
        context = instance.fetch_post_context(ident, ancestors=False, descendants=True)
        print_posts(context.descendants, hide_in_reply_to=True)

def print_posts(posts, *, pinned=False, hide_in_reply_to=False):
    n = 0
    for n, post in enumerate(posts, start=1):
        print()
        print('-' * text_width)
        print()
        print_post(post, pinned=pinned, hide_in_reply_to=hide_in_reply_to)
    return n

def normalize_lang(lang):
    if lang is None:
        return 'en'
    if lang.startswith('en-'):
        return 'en'
    return lang

paperclip = Symbol('paperclip')

def print_post(post, *, pinned=False, hide_in_reply_to=False):
    url = post.url or post.uri
    if url:
        print('Location:', fmt_url(url))
    if post.in_reply_to_url and not hide_in_reply_to:
        print('In-Reply-To:', fmt_url(post.in_reply_to_url))
    if pinned:
        print('Pinned: yes')
    if post.account:
        # FIXME in Pleroma?
        # Why is the account information missing
        # for some reblogged posts?
        print('From:', fmt_user(post.account))
    date_comment = []
    if post.edited_at:
        date_comment = 'edited ' + fmt_date(post.edited_at)
        date_comment = [f'({date_comment})']
    print('Date:', fmt_date(post.created_at), *date_comment)
    if normalize_lang(post.language) != 'en':
        print('Language:', post.language)
    if post.reblog:
        print('Reblog: yes')
    print()
    if post.reblog:
        print_post(post.reblog)
    else:
        text = fmt_html(post.content)
        print(text)
    print()
    for att in post.media_attachments or ():
        print(paperclip, fmt_url(att.url))
        print()
        text = att.description or ''
        indent = ' ' * (1 + paperclip.width)
        text = wrap_text(text, indent=indent)
        for line in text:
            print(line)
        print()

def wrap_text(text, indent=''):
    text = text.splitlines()
    for line in text:
        line = textwrap.wrap(line,
            width=text_width,
            initial_indent=indent,
            subsequent_indent=indent,
            break_long_words=False,
        )
        yield str.join('\n', line)

def main():
    try:
        xmain()
    except BrokenPipeError:
        signal.signal(signal.SIGPIPE, signal.SIG_DFL)
        os.kill(os.getpid(), signal.SIGPIPE)
        raise

if __name__ == '__main__':
    main()

# vim:ts=4 sts=4 sw=4 et
